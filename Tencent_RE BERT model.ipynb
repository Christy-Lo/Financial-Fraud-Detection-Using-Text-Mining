{"cells":[{"cell_type":"markdown","metadata":{"id":"8jEeZ9vU227n"},"source":["This notebook is implementing the github repositary \"BERT-Relation-Extraction\" from plkmo (link:https://github.com/plkmo/BERT-Relation-Extraction)"]},{"cell_type":"markdown","metadata":{"id":"exG7jPDgPpoq"},"source":["This notebook will guide you how to run the trained model.<br><br>\n","<b>[IMPORTANT]</b>: Before going into the code below, add the shortcut of the Project folder to your drive:<br>\n","1. Right-click our project folder \n","2. Click \"Add shortcut to Drive\"\n","3. Select \"My Drive\" and click \"ADD SHORTCUT\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21500,"status":"ok","timestamp":1640166514513,"user":{"displayName":"Tsz Chung Yeung","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17478010975126361284"},"user_tz":-480},"id":"HKqwBS-G2pv7","outputId":"cf38e41d-19e8-4d61-d9e0-4954da6b6a35"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Connect to Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1140705,"status":"ok","timestamp":1640167675974,"user":{"displayName":"Tsz Chung Yeung","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17478010975126361284"},"user_tz":-480},"id":"QhsK49jo3pAK","outputId":"28b1df3b-f85e-419d-8b8e-c5652add1194"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=cfd19263ce99ae7501a95906f50047f672b86a4d57cb38a94333f8405059ef4c\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Collecting boto3\n","  Downloading boto3-1.20.26-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 13.0 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting botocore<1.24.0,>=1.23.26\n","  Downloading botocore-1.23.26-py3-none-any.whl (8.5 MB)\n","\u001b[K     |████████████████████████████████| 8.5 MB 46.2 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.26->boto3) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 50.9 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.26->boto3) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.20.26 botocore-1.23.26 jmespath-0.10.0 s3transfer-0.5.0 urllib3-1.26.7\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 12.2 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Installing collected packages: urllib3\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.26.7\n","    Uninstalling urllib3-1.26.7:\n","      Successfully uninstalled urllib3-1.26.7\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed urllib3-1.25.11\n","Collecting en_core_web_lg==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n","\u001b[K     |████████████████████████████████| 827.9 MB 658 kB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.8.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.11)\n","Building wheels for collected packages: en-core-web-lg\n","  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=a75086f01ac3cd4374754144b0c52726ff47f3f000043724139fe3941ed79a94\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ldg6f_mp/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n","Successfully built en-core-web-lg\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n"]}],"source":["# Install the required libs\n","!pip install seqeval\n","!pip install boto3\n","!pip install spacy\n","#!pip3 install torch==1.10.0+cu102 torchvision==0.11.1+cu102 torchaudio===0.10.0+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html\n","!python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1528911,"status":"ok","timestamp":1640169204878,"user":{"displayName":"Tsz Chung Yeung","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17478010975126361284"},"user_tz":-480},"id":"XaZkTdYyWTtJ","outputId":"3110dbe5-d64e-4373-92fb-a82469c8a996"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1jqlh-dTe4b6_AZhAYDBHkKjWweeOh9HE/COMP4805 Project/Go to Street/__Tencent/BERT-Relation-Extraction\n","12/22/2021 10:08:15 AM [INFO]: TensorFlow version 2.7.0 available.\n","12/22/2021 10:08:15 AM [INFO]: PyTorch version 1.10.0+cu111 available.\n","12/22/2021 10:08:15 AM [INFO]: Pre-trained blanks tokenizer not found, initializing new tokenizer...\n","12/22/2021 10:08:16 AM [INFO]: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpjzc21nbv\n","100% 231508/231508 [00:00<00:00, 917110.82B/s]\n","12/22/2021 10:08:16 AM [INFO]: copying /tmp/tmpjzc21nbv to cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","12/22/2021 10:08:16 AM [INFO]: creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","12/22/2021 10:08:16 AM [INFO]: removing temp file /tmp/tmpjzc21nbv\n","12/22/2021 10:08:16 AM [INFO]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","12/22/2021 10:08:16 AM [INFO]: Adding [E1] to the vocabulary\n","12/22/2021 10:08:16 AM [INFO]: Adding [/E1] to the vocabulary\n","12/22/2021 10:08:16 AM [INFO]: Adding [E2] to the vocabulary\n","12/22/2021 10:08:16 AM [INFO]: Adding [/E2] to the vocabulary\n","12/22/2021 10:08:16 AM [INFO]: Adding [BLANK] to the vocabulary\n","12/22/2021 10:08:16 AM [INFO]: Saved BERT tokenizer at ./data/BERT_tokenizer.pkl\n","12/22/2021 10:08:16 AM [INFO]: Reading training file /content/drive/MyDrive/COMP4805 Project/Go to Street/__Tencent/train_data_dm1.txt...\n","12/22/2021 10:08:18 AM [INFO]: Reading test file /content/drive/MyDrive/COMP4805 Project/Go to Street/__Tencent/test_data_dm1.txt...\n","12/22/2021 10:08:18 AM [INFO]: Mapping relations to IDs...\n","100% 2400/2400 [00:00<00:00, 1755857.25it/s]\n","prog-bar: 100% 266/266 [00:00<00:00, 32935.35it/s]\n","prog-bar: 100% 2400/2400 [00:00<00:00, 98614.10it/s]\n","12/22/2021 10:08:18 AM [INFO]: Finished and saved!\n","12/22/2021 10:08:18 AM [INFO]: Tokenizing data...\n","prog-bar: 100% 2400/2400 [00:03<00:00, 660.69it/s]\n","prog-bar: 100% 2400/2400 [00:00<00:00, 46084.92it/s]\n","\n","Invalid rows/total: 0/2400\n","12/22/2021 10:08:22 AM [INFO]: NumExpr defaulting to 2 threads.\n","12/22/2021 10:08:22 AM [INFO]: Tokenizing data...\n","prog-bar: 100% 266/266 [00:00<00:00, 635.77it/s]\n","prog-bar: 100% 266/266 [00:00<00:00, 44502.79it/s]\n","\n","Invalid rows/total: 0/266\n","12/22/2021 10:08:22 AM [INFO]: Loaded 2400 Training samples.\n","12/22/2021 10:08:25 AM [INFO]: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp08u7j8mp\n","100% 433/433 [00:00<00:00, 429447.54B/s]\n","12/22/2021 10:08:25 AM [INFO]: copying /tmp/tmp08u7j8mp to cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","12/22/2021 10:08:25 AM [INFO]: creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","12/22/2021 10:08:25 AM [INFO]: removing temp file /tmp/tmp08u7j8mp\n","12/22/2021 10:08:25 AM [INFO]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","12/22/2021 10:08:25 AM [INFO]: Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","12/22/2021 10:08:26 AM [INFO]: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpq4br_7yv\n","100% 440473133/440473133 [00:19<00:00, 22301674.14B/s]\n","12/22/2021 10:08:46 AM [INFO]: copying /tmp/tmpq4br_7yv to cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","12/22/2021 10:08:47 AM [INFO]: creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","12/22/2021 10:08:47 AM [INFO]: removing temp file /tmp/tmpq4br_7yv\n","12/22/2021 10:08:47 AM [INFO]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","Model config:  {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","12/22/2021 10:08:50 AM [INFO]: Weights of BertModel not initialized from pretrained model: ['bert.classification_layer.weight', 'bert.classification_layer.bias']\n","12/22/2021 10:10:03 AM [INFO]: FREEZING MOST HIDDEN LAYERS...\n","[FROZE]: embeddings.word_embeddings.weight\n","[FROZE]: embeddings.position_embeddings.weight\n","[FROZE]: embeddings.token_type_embeddings.weight\n","[FROZE]: embeddings.LayerNorm.weight\n","[FROZE]: embeddings.LayerNorm.bias\n","[FROZE]: encoder.layer.0.attention.self.query.weight\n","[FROZE]: encoder.layer.0.attention.self.query.bias\n","[FROZE]: encoder.layer.0.attention.self.key.weight\n","[FROZE]: encoder.layer.0.attention.self.key.bias\n","[FROZE]: encoder.layer.0.attention.self.value.weight\n","[FROZE]: encoder.layer.0.attention.self.value.bias\n","[FROZE]: encoder.layer.0.attention.output.dense.weight\n","[FROZE]: encoder.layer.0.attention.output.dense.bias\n","[FROZE]: encoder.layer.0.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.0.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.0.intermediate.dense.weight\n","[FROZE]: encoder.layer.0.intermediate.dense.bias\n","[FROZE]: encoder.layer.0.output.dense.weight\n","[FROZE]: encoder.layer.0.output.dense.bias\n","[FROZE]: encoder.layer.0.output.LayerNorm.weight\n","[FROZE]: encoder.layer.0.output.LayerNorm.bias\n","[FROZE]: encoder.layer.1.attention.self.query.weight\n","[FROZE]: encoder.layer.1.attention.self.query.bias\n","[FROZE]: encoder.layer.1.attention.self.key.weight\n","[FROZE]: encoder.layer.1.attention.self.key.bias\n","[FROZE]: encoder.layer.1.attention.self.value.weight\n","[FROZE]: encoder.layer.1.attention.self.value.bias\n","[FROZE]: encoder.layer.1.attention.output.dense.weight\n","[FROZE]: encoder.layer.1.attention.output.dense.bias\n","[FROZE]: encoder.layer.1.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.1.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.1.intermediate.dense.weight\n","[FROZE]: encoder.layer.1.intermediate.dense.bias\n","[FROZE]: encoder.layer.1.output.dense.weight\n","[FROZE]: encoder.layer.1.output.dense.bias\n","[FROZE]: encoder.layer.1.output.LayerNorm.weight\n","[FROZE]: encoder.layer.1.output.LayerNorm.bias\n","[FROZE]: encoder.layer.2.attention.self.query.weight\n","[FROZE]: encoder.layer.2.attention.self.query.bias\n","[FROZE]: encoder.layer.2.attention.self.key.weight\n","[FROZE]: encoder.layer.2.attention.self.key.bias\n","[FROZE]: encoder.layer.2.attention.self.value.weight\n","[FROZE]: encoder.layer.2.attention.self.value.bias\n","[FROZE]: encoder.layer.2.attention.output.dense.weight\n","[FROZE]: encoder.layer.2.attention.output.dense.bias\n","[FROZE]: encoder.layer.2.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.2.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.2.intermediate.dense.weight\n","[FROZE]: encoder.layer.2.intermediate.dense.bias\n","[FROZE]: encoder.layer.2.output.dense.weight\n","[FROZE]: encoder.layer.2.output.dense.bias\n","[FROZE]: encoder.layer.2.output.LayerNorm.weight\n","[FROZE]: encoder.layer.2.output.LayerNorm.bias\n","[FROZE]: encoder.layer.3.attention.self.query.weight\n","[FROZE]: encoder.layer.3.attention.self.query.bias\n","[FROZE]: encoder.layer.3.attention.self.key.weight\n","[FROZE]: encoder.layer.3.attention.self.key.bias\n","[FROZE]: encoder.layer.3.attention.self.value.weight\n","[FROZE]: encoder.layer.3.attention.self.value.bias\n","[FROZE]: encoder.layer.3.attention.output.dense.weight\n","[FROZE]: encoder.layer.3.attention.output.dense.bias\n","[FROZE]: encoder.layer.3.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.3.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.3.intermediate.dense.weight\n","[FROZE]: encoder.layer.3.intermediate.dense.bias\n","[FROZE]: encoder.layer.3.output.dense.weight\n","[FROZE]: encoder.layer.3.output.dense.bias\n","[FROZE]: encoder.layer.3.output.LayerNorm.weight\n","[FROZE]: encoder.layer.3.output.LayerNorm.bias\n","[FROZE]: encoder.layer.4.attention.self.query.weight\n","[FROZE]: encoder.layer.4.attention.self.query.bias\n","[FROZE]: encoder.layer.4.attention.self.key.weight\n","[FROZE]: encoder.layer.4.attention.self.key.bias\n","[FROZE]: encoder.layer.4.attention.self.value.weight\n","[FROZE]: encoder.layer.4.attention.self.value.bias\n","[FROZE]: encoder.layer.4.attention.output.dense.weight\n","[FROZE]: encoder.layer.4.attention.output.dense.bias\n","[FROZE]: encoder.layer.4.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.4.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.4.intermediate.dense.weight\n","[FROZE]: encoder.layer.4.intermediate.dense.bias\n","[FROZE]: encoder.layer.4.output.dense.weight\n","[FROZE]: encoder.layer.4.output.dense.bias\n","[FROZE]: encoder.layer.4.output.LayerNorm.weight\n","[FROZE]: encoder.layer.4.output.LayerNorm.bias\n","[FROZE]: encoder.layer.5.attention.self.query.weight\n","[FROZE]: encoder.layer.5.attention.self.query.bias\n","[FROZE]: encoder.layer.5.attention.self.key.weight\n","[FROZE]: encoder.layer.5.attention.self.key.bias\n","[FROZE]: encoder.layer.5.attention.self.value.weight\n","[FROZE]: encoder.layer.5.attention.self.value.bias\n","[FROZE]: encoder.layer.5.attention.output.dense.weight\n","[FROZE]: encoder.layer.5.attention.output.dense.bias\n","[FROZE]: encoder.layer.5.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.5.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.5.intermediate.dense.weight\n","[FROZE]: encoder.layer.5.intermediate.dense.bias\n","[FROZE]: encoder.layer.5.output.dense.weight\n","[FROZE]: encoder.layer.5.output.dense.bias\n","[FROZE]: encoder.layer.5.output.LayerNorm.weight\n","[FROZE]: encoder.layer.5.output.LayerNorm.bias\n","[FROZE]: encoder.layer.6.attention.self.query.weight\n","[FROZE]: encoder.layer.6.attention.self.query.bias\n","[FROZE]: encoder.layer.6.attention.self.key.weight\n","[FROZE]: encoder.layer.6.attention.self.key.bias\n","[FROZE]: encoder.layer.6.attention.self.value.weight\n","[FROZE]: encoder.layer.6.attention.self.value.bias\n","[FROZE]: encoder.layer.6.attention.output.dense.weight\n","[FROZE]: encoder.layer.6.attention.output.dense.bias\n","[FROZE]: encoder.layer.6.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.6.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.6.intermediate.dense.weight\n","[FROZE]: encoder.layer.6.intermediate.dense.bias\n","[FROZE]: encoder.layer.6.output.dense.weight\n","[FROZE]: encoder.layer.6.output.dense.bias\n","[FROZE]: encoder.layer.6.output.LayerNorm.weight\n","[FROZE]: encoder.layer.6.output.LayerNorm.bias\n","[FROZE]: encoder.layer.7.attention.self.query.weight\n","[FROZE]: encoder.layer.7.attention.self.query.bias\n","[FROZE]: encoder.layer.7.attention.self.key.weight\n","[FROZE]: encoder.layer.7.attention.self.key.bias\n","[FROZE]: encoder.layer.7.attention.self.value.weight\n","[FROZE]: encoder.layer.7.attention.self.value.bias\n","[FROZE]: encoder.layer.7.attention.output.dense.weight\n","[FROZE]: encoder.layer.7.attention.output.dense.bias\n","[FROZE]: encoder.layer.7.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.7.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.7.intermediate.dense.weight\n","[FROZE]: encoder.layer.7.intermediate.dense.bias\n","[FROZE]: encoder.layer.7.output.dense.weight\n","[FROZE]: encoder.layer.7.output.dense.bias\n","[FROZE]: encoder.layer.7.output.LayerNorm.weight\n","[FROZE]: encoder.layer.7.output.LayerNorm.bias\n","[FROZE]: encoder.layer.8.attention.self.query.weight\n","[FROZE]: encoder.layer.8.attention.self.query.bias\n","[FROZE]: encoder.layer.8.attention.self.key.weight\n","[FROZE]: encoder.layer.8.attention.self.key.bias\n","[FROZE]: encoder.layer.8.attention.self.value.weight\n","[FROZE]: encoder.layer.8.attention.self.value.bias\n","[FROZE]: encoder.layer.8.attention.output.dense.weight\n","[FROZE]: encoder.layer.8.attention.output.dense.bias\n","[FROZE]: encoder.layer.8.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.8.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.8.intermediate.dense.weight\n","[FROZE]: encoder.layer.8.intermediate.dense.bias\n","[FROZE]: encoder.layer.8.output.dense.weight\n","[FROZE]: encoder.layer.8.output.dense.bias\n","[FROZE]: encoder.layer.8.output.LayerNorm.weight\n","[FROZE]: encoder.layer.8.output.LayerNorm.bias\n","[FROZE]: encoder.layer.9.attention.self.query.weight\n","[FROZE]: encoder.layer.9.attention.self.query.bias\n","[FROZE]: encoder.layer.9.attention.self.key.weight\n","[FROZE]: encoder.layer.9.attention.self.key.bias\n","[FROZE]: encoder.layer.9.attention.self.value.weight\n","[FROZE]: encoder.layer.9.attention.self.value.bias\n","[FROZE]: encoder.layer.9.attention.output.dense.weight\n","[FROZE]: encoder.layer.9.attention.output.dense.bias\n","[FROZE]: encoder.layer.9.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.9.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.9.intermediate.dense.weight\n","[FROZE]: encoder.layer.9.intermediate.dense.bias\n","[FROZE]: encoder.layer.9.output.dense.weight\n","[FROZE]: encoder.layer.9.output.dense.bias\n","[FROZE]: encoder.layer.9.output.LayerNorm.weight\n","[FROZE]: encoder.layer.9.output.LayerNorm.bias\n","[FROZE]: encoder.layer.10.attention.self.query.weight\n","[FROZE]: encoder.layer.10.attention.self.query.bias\n","[FROZE]: encoder.layer.10.attention.self.key.weight\n","[FROZE]: encoder.layer.10.attention.self.key.bias\n","[FROZE]: encoder.layer.10.attention.self.value.weight\n","[FROZE]: encoder.layer.10.attention.self.value.bias\n","[FROZE]: encoder.layer.10.attention.output.dense.weight\n","[FROZE]: encoder.layer.10.attention.output.dense.bias\n","[FROZE]: encoder.layer.10.attention.output.LayerNorm.weight\n","[FROZE]: encoder.layer.10.attention.output.LayerNorm.bias\n","[FROZE]: encoder.layer.10.intermediate.dense.weight\n","[FROZE]: encoder.layer.10.intermediate.dense.bias\n","[FROZE]: encoder.layer.10.output.dense.weight\n","[FROZE]: encoder.layer.10.output.dense.bias\n","[FROZE]: encoder.layer.10.output.LayerNorm.weight\n","[FROZE]: encoder.layer.10.output.LayerNorm.bias\n","[FREE]: encoder.layer.11.attention.self.query.weight\n","[FREE]: encoder.layer.11.attention.self.query.bias\n","[FREE]: encoder.layer.11.attention.self.key.weight\n","[FREE]: encoder.layer.11.attention.self.key.bias\n","[FREE]: encoder.layer.11.attention.self.value.weight\n","[FREE]: encoder.layer.11.attention.self.value.bias\n","[FREE]: encoder.layer.11.attention.output.dense.weight\n","[FREE]: encoder.layer.11.attention.output.dense.bias\n","[FREE]: encoder.layer.11.attention.output.LayerNorm.weight\n","[FREE]: encoder.layer.11.attention.output.LayerNorm.bias\n","[FREE]: encoder.layer.11.intermediate.dense.weight\n","[FREE]: encoder.layer.11.intermediate.dense.bias\n","[FREE]: encoder.layer.11.output.dense.weight\n","[FREE]: encoder.layer.11.output.dense.bias\n","[FREE]: encoder.layer.11.output.LayerNorm.weight\n","[FREE]: encoder.layer.11.output.LayerNorm.bias\n","[FREE]: pooler.dense.weight\n","[FREE]: pooler.dense.bias\n","[FREE]: classification_layer.weight\n","[FREE]: classification_layer.bias\n","12/22/2021 10:10:03 AM [INFO]: Starting training process...\n","[Epoch: 1,   224/ 2400 points] total loss, accuracy per batch: 2.827, 0.214\n","[Epoch: 1,   448/ 2400 points] total loss, accuracy per batch: 2.208, 0.585\n","[Epoch: 1,   672/ 2400 points] total loss, accuracy per batch: 1.977, 0.451\n","[Epoch: 1,   896/ 2400 points] total loss, accuracy per batch: 1.777, 0.504\n","[Epoch: 1,  1120/ 2400 points] total loss, accuracy per batch: 1.522, 0.522\n","[Epoch: 1,  1344/ 2400 points] total loss, accuracy per batch: 1.540, 0.571\n","[Epoch: 1,  1568/ 2400 points] total loss, accuracy per batch: 1.389, 0.598\n","[Epoch: 1,  1792/ 2400 points] total loss, accuracy per batch: 1.416, 0.621\n","[Epoch: 1,  2016/ 2400 points] total loss, accuracy per batch: 1.314, 0.661\n","[Epoch: 1,  2240/ 2400 points] total loss, accuracy per batch: 1.124, 0.670\n","12/22/2021 10:11:02 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.51it/s]\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 1 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 5 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 3 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 2 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 10 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 11 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 13 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 4 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 6 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 9 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 8 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","12/22/2021 10:11:08 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:11:08 AM [INFO]:   accuracy = 0.6652777777777777\n","12/22/2021 10:11:08 AM [INFO]:   f1 = 0.0\n","12/22/2021 10:11:08 AM [INFO]:   precision = 0.0\n","12/22/2021 10:11:08 AM [INFO]:   recall = 0.0\n","Epoch finished, took 64.60 seconds.\n","Losses at Epoch 1: 1.7093487\n","Train accuracy at Epoch 1: 0.5397321\n","Test f1 at Epoch 1: 0.0000000\n","[Epoch: 2,   224/ 2400 points] total loss, accuracy per batch: 1.065, 0.701\n","[Epoch: 2,   448/ 2400 points] total loss, accuracy per batch: 1.026, 0.728\n","[Epoch: 2,   672/ 2400 points] total loss, accuracy per batch: 1.043, 0.692\n","[Epoch: 2,   896/ 2400 points] total loss, accuracy per batch: 1.162, 0.661\n","[Epoch: 2,  1120/ 2400 points] total loss, accuracy per batch: 1.023, 0.665\n","[Epoch: 2,  1344/ 2400 points] total loss, accuracy per batch: 0.922, 0.759\n","[Epoch: 2,  1568/ 2400 points] total loss, accuracy per batch: 0.889, 0.714\n","[Epoch: 2,  1792/ 2400 points] total loss, accuracy per batch: 0.823, 0.781\n","[Epoch: 2,  2016/ 2400 points] total loss, accuracy per batch: 0.880, 0.741\n","[Epoch: 2,  2240/ 2400 points] total loss, accuracy per batch: 0.897, 0.732\n","12/22/2021 10:12:12 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.54it/s]\n","12/22/2021 10:12:18 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:12:18 AM [INFO]:   accuracy = 0.7284722222222223\n","12/22/2021 10:12:18 AM [INFO]:   f1 = 0.06896551724137931\n","12/22/2021 10:12:18 AM [INFO]:   precision = 0.16666666666666666\n","12/22/2021 10:12:18 AM [INFO]:   recall = 0.043478260869565216\n","Epoch finished, took 65.68 seconds.\n","Losses at Epoch 2: 0.9729615\n","Train accuracy at Epoch 2: 0.7174107\n","Test f1 at Epoch 2: 0.0689655\n","[Epoch: 3,   224/ 2400 points] total loss, accuracy per batch: 0.815, 0.746\n","[Epoch: 3,   448/ 2400 points] total loss, accuracy per batch: 0.892, 0.741\n","[Epoch: 3,   672/ 2400 points] total loss, accuracy per batch: 0.763, 0.795\n","[Epoch: 3,   896/ 2400 points] total loss, accuracy per batch: 0.807, 0.763\n","[Epoch: 3,  1120/ 2400 points] total loss, accuracy per batch: 0.669, 0.790\n","[Epoch: 3,  1344/ 2400 points] total loss, accuracy per batch: 0.578, 0.830\n","[Epoch: 3,  1568/ 2400 points] total loss, accuracy per batch: 0.668, 0.812\n","[Epoch: 3,  1792/ 2400 points] total loss, accuracy per batch: 0.656, 0.804\n","[Epoch: 3,  2016/ 2400 points] total loss, accuracy per batch: 0.792, 0.759\n","[Epoch: 3,  2240/ 2400 points] total loss, accuracy per batch: 0.913, 0.750\n","12/22/2021 10:13:20 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:06<00:00,  1.49it/s]\n","12/22/2021 10:13:26 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:13:26 AM [INFO]:   accuracy = 0.7909722222222223\n","12/22/2021 10:13:26 AM [INFO]:   f1 = 0.15\n","12/22/2021 10:13:26 AM [INFO]:   precision = 0.1875\n","12/22/2021 10:13:26 AM [INFO]:   recall = 0.125\n","Epoch finished, took 63.58 seconds.\n","Losses at Epoch 3: 0.7551882\n","Train accuracy at Epoch 3: 0.7790179\n","Test f1 at Epoch 3: 0.1500000\n","[Epoch: 4,   224/ 2400 points] total loss, accuracy per batch: 0.796, 0.746\n","[Epoch: 4,   448/ 2400 points] total loss, accuracy per batch: 0.663, 0.804\n","[Epoch: 4,   672/ 2400 points] total loss, accuracy per batch: 0.713, 0.777\n","[Epoch: 4,   896/ 2400 points] total loss, accuracy per batch: 0.621, 0.817\n","[Epoch: 4,  1120/ 2400 points] total loss, accuracy per batch: 0.598, 0.830\n","[Epoch: 4,  1344/ 2400 points] total loss, accuracy per batch: 0.595, 0.844\n","[Epoch: 4,  1568/ 2400 points] total loss, accuracy per batch: 0.602, 0.812\n","[Epoch: 4,  1792/ 2400 points] total loss, accuracy per batch: 0.539, 0.839\n","[Epoch: 4,  2016/ 2400 points] total loss, accuracy per batch: 0.623, 0.817\n","[Epoch: 4,  2240/ 2400 points] total loss, accuracy per batch: 0.587, 0.821\n","12/22/2021 10:14:30 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:06<00:00,  1.50it/s]\n","12/22/2021 10:14:36 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:14:36 AM [INFO]:   accuracy = 0.8076388888888889\n","12/22/2021 10:14:36 AM [INFO]:   f1 = 0.19354838709677416\n","12/22/2021 10:14:36 AM [INFO]:   precision = 0.375\n","12/22/2021 10:14:36 AM [INFO]:   recall = 0.13043478260869565\n","Epoch finished, took 65.80 seconds.\n","Losses at Epoch 4: 0.6336020\n","Train accuracy at Epoch 4: 0.8107143\n","Test f1 at Epoch 4: 0.1935484\n","[Epoch: 5,   224/ 2400 points] total loss, accuracy per batch: 0.671, 0.804\n","[Epoch: 5,   448/ 2400 points] total loss, accuracy per batch: 0.570, 0.817\n","[Epoch: 5,   672/ 2400 points] total loss, accuracy per batch: 0.567, 0.799\n","[Epoch: 5,   896/ 2400 points] total loss, accuracy per batch: 0.578, 0.817\n","[Epoch: 5,  1120/ 2400 points] total loss, accuracy per batch: 0.562, 0.808\n","[Epoch: 5,  1344/ 2400 points] total loss, accuracy per batch: 0.737, 0.790\n","[Epoch: 5,  1568/ 2400 points] total loss, accuracy per batch: 0.503, 0.866\n","[Epoch: 5,  1792/ 2400 points] total loss, accuracy per batch: 0.476, 0.866\n","[Epoch: 5,  2016/ 2400 points] total loss, accuracy per batch: 0.526, 0.844\n","[Epoch: 5,  2240/ 2400 points] total loss, accuracy per batch: 0.537, 0.848\n","12/22/2021 10:15:39 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.57it/s]\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 12 seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","12/22/2021 10:15:45 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:15:45 AM [INFO]:   accuracy = 0.8215277777777777\n","12/22/2021 10:15:45 AM [INFO]:   f1 = 0.25\n","12/22/2021 10:15:45 AM [INFO]:   precision = 0.5\n","12/22/2021 10:15:45 AM [INFO]:   recall = 0.16666666666666666\n","Epoch finished, took 64.10 seconds.\n","Losses at Epoch 5: 0.5728274\n","Train accuracy at Epoch 5: 0.8258929\n","Test f1 at Epoch 5: 0.2500000\n","[Epoch: 6,   224/ 2400 points] total loss, accuracy per batch: 0.475, 0.839\n","[Epoch: 6,   448/ 2400 points] total loss, accuracy per batch: 0.563, 0.839\n","[Epoch: 6,   672/ 2400 points] total loss, accuracy per batch: 0.559, 0.817\n","[Epoch: 6,   896/ 2400 points] total loss, accuracy per batch: 0.528, 0.830\n","[Epoch: 6,  1120/ 2400 points] total loss, accuracy per batch: 0.368, 0.902\n","[Epoch: 6,  1344/ 2400 points] total loss, accuracy per batch: 0.503, 0.826\n","[Epoch: 6,  1568/ 2400 points] total loss, accuracy per batch: 0.539, 0.830\n","[Epoch: 6,  1792/ 2400 points] total loss, accuracy per batch: 0.579, 0.830\n","[Epoch: 6,  2016/ 2400 points] total loss, accuracy per batch: 0.533, 0.821\n","[Epoch: 6,  2240/ 2400 points] total loss, accuracy per batch: 0.478, 0.853\n","12/22/2021 10:16:48 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.72it/s]\n","12/22/2021 10:16:53 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:16:53 AM [INFO]:   accuracy = 0.8291666666666667\n","12/22/2021 10:16:53 AM [INFO]:   f1 = 0.12500000000000003\n","12/22/2021 10:16:53 AM [INFO]:   precision = 0.2\n","12/22/2021 10:16:53 AM [INFO]:   recall = 0.09090909090909091\n","Epoch finished, took 63.52 seconds.\n","Losses at Epoch 6: 0.5125597\n","Train accuracy at Epoch 6: 0.8388393\n","Test f1 at Epoch 6: 0.1250000\n","[Epoch: 7,   224/ 2400 points] total loss, accuracy per batch: 0.478, 0.848\n","[Epoch: 7,   448/ 2400 points] total loss, accuracy per batch: 0.540, 0.844\n","[Epoch: 7,   672/ 2400 points] total loss, accuracy per batch: 0.506, 0.844\n","[Epoch: 7,   896/ 2400 points] total loss, accuracy per batch: 0.417, 0.853\n","[Epoch: 7,  1120/ 2400 points] total loss, accuracy per batch: 0.406, 0.888\n","[Epoch: 7,  1344/ 2400 points] total loss, accuracy per batch: 0.422, 0.875\n","[Epoch: 7,  1568/ 2400 points] total loss, accuracy per batch: 0.495, 0.853\n","[Epoch: 7,  1792/ 2400 points] total loss, accuracy per batch: 0.428, 0.879\n","[Epoch: 7,  2016/ 2400 points] total loss, accuracy per batch: 0.504, 0.812\n","[Epoch: 7,  2240/ 2400 points] total loss, accuracy per batch: 0.459, 0.830\n","12/22/2021 10:17:56 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.59it/s]\n","12/22/2021 10:18:01 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:18:01 AM [INFO]:   accuracy = 0.8527777777777777\n","12/22/2021 10:18:01 AM [INFO]:   f1 = 0.25\n","12/22/2021 10:18:01 AM [INFO]:   precision = 0.5\n","12/22/2021 10:18:01 AM [INFO]:   recall = 0.16666666666666666\n","Epoch finished, took 62.60 seconds.\n","Losses at Epoch 7: 0.4655859\n","Train accuracy at Epoch 7: 0.8526786\n","Test f1 at Epoch 7: 0.2500000\n","[Epoch: 8,   224/ 2400 points] total loss, accuracy per batch: 0.414, 0.875\n","[Epoch: 8,   448/ 2400 points] total loss, accuracy per batch: 0.449, 0.853\n","[Epoch: 8,   672/ 2400 points] total loss, accuracy per batch: 0.411, 0.875\n","[Epoch: 8,   896/ 2400 points] total loss, accuracy per batch: 0.440, 0.875\n","[Epoch: 8,  1120/ 2400 points] total loss, accuracy per batch: 0.397, 0.879\n","[Epoch: 8,  1344/ 2400 points] total loss, accuracy per batch: 0.471, 0.862\n","[Epoch: 8,  1568/ 2400 points] total loss, accuracy per batch: 0.490, 0.857\n","[Epoch: 8,  1792/ 2400 points] total loss, accuracy per batch: 0.448, 0.848\n","[Epoch: 8,  2016/ 2400 points] total loss, accuracy per batch: 0.447, 0.848\n","[Epoch: 8,  2240/ 2400 points] total loss, accuracy per batch: 0.475, 0.871\n","12/22/2021 10:19:04 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.56it/s]\n","12/22/2021 10:19:09 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:19:09 AM [INFO]:   accuracy = 0.8451388888888889\n","12/22/2021 10:19:09 AM [INFO]:   f1 = 0.3870967741935483\n","12/22/2021 10:19:09 AM [INFO]:   precision = 0.75\n","12/22/2021 10:19:09 AM [INFO]:   recall = 0.2608695652173913\n","Epoch finished, took 63.69 seconds.\n","Losses at Epoch 8: 0.4440956\n","Train accuracy at Epoch 8: 0.8642857\n","Test f1 at Epoch 8: 0.3870968\n","[Epoch: 9,   224/ 2400 points] total loss, accuracy per batch: 0.384, 0.857\n","[Epoch: 9,   448/ 2400 points] total loss, accuracy per batch: 0.412, 0.884\n","[Epoch: 9,   672/ 2400 points] total loss, accuracy per batch: 0.394, 0.879\n","[Epoch: 9,   896/ 2400 points] total loss, accuracy per batch: 0.405, 0.866\n","[Epoch: 9,  1120/ 2400 points] total loss, accuracy per batch: 0.416, 0.862\n","[Epoch: 9,  1344/ 2400 points] total loss, accuracy per batch: 0.398, 0.866\n","[Epoch: 9,  1568/ 2400 points] total loss, accuracy per batch: 0.443, 0.857\n","[Epoch: 9,  1792/ 2400 points] total loss, accuracy per batch: 0.437, 0.875\n","[Epoch: 9,  2016/ 2400 points] total loss, accuracy per batch: 0.334, 0.911\n","[Epoch: 9,  2240/ 2400 points] total loss, accuracy per batch: 0.418, 0.875\n","12/22/2021 10:20:14 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.62it/s]\n","12/22/2021 10:20:19 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:20:19 AM [INFO]:   accuracy = 0.8458333333333333\n","12/22/2021 10:20:19 AM [INFO]:   f1 = 0.29411764705882354\n","12/22/2021 10:20:19 AM [INFO]:   precision = 0.5\n","12/22/2021 10:20:19 AM [INFO]:   recall = 0.20833333333333334\n","Epoch finished, took 65.18 seconds.\n","Losses at Epoch 9: 0.4042539\n","Train accuracy at Epoch 9: 0.8732143\n","Test f1 at Epoch 9: 0.2941176\n","[Epoch: 10,   224/ 2400 points] total loss, accuracy per batch: 0.432, 0.888\n","[Epoch: 10,   448/ 2400 points] total loss, accuracy per batch: 0.394, 0.871\n","[Epoch: 10,   672/ 2400 points] total loss, accuracy per batch: 0.418, 0.871\n","[Epoch: 10,   896/ 2400 points] total loss, accuracy per batch: 0.397, 0.879\n","[Epoch: 10,  1120/ 2400 points] total loss, accuracy per batch: 0.404, 0.884\n","[Epoch: 10,  1344/ 2400 points] total loss, accuracy per batch: 0.409, 0.871\n","[Epoch: 10,  1568/ 2400 points] total loss, accuracy per batch: 0.302, 0.915\n","[Epoch: 10,  1792/ 2400 points] total loss, accuracy per batch: 0.446, 0.871\n","[Epoch: 10,  2016/ 2400 points] total loss, accuracy per batch: 0.314, 0.897\n","[Epoch: 10,  2240/ 2400 points] total loss, accuracy per batch: 0.287, 0.915\n","12/22/2021 10:21:24 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.59it/s]\n","12/22/2021 10:21:29 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:21:29 AM [INFO]:   accuracy = 0.8715277777777778\n","12/22/2021 10:21:29 AM [INFO]:   f1 = 0.2\n","12/22/2021 10:21:29 AM [INFO]:   precision = 0.5\n","12/22/2021 10:21:29 AM [INFO]:   recall = 0.125\n","Epoch finished, took 65.42 seconds.\n","Losses at Epoch 10: 0.3803434\n","Train accuracy at Epoch 10: 0.8861607\n","Test f1 at Epoch 10: 0.2000000\n","[Epoch: 11,   224/ 2400 points] total loss, accuracy per batch: 0.333, 0.888\n","[Epoch: 11,   448/ 2400 points] total loss, accuracy per batch: 0.407, 0.888\n","[Epoch: 11,   672/ 2400 points] total loss, accuracy per batch: 0.298, 0.920\n","[Epoch: 11,   896/ 2400 points] total loss, accuracy per batch: 0.368, 0.862\n","[Epoch: 11,  1120/ 2400 points] total loss, accuracy per batch: 0.356, 0.888\n","[Epoch: 11,  1344/ 2400 points] total loss, accuracy per batch: 0.370, 0.866\n","[Epoch: 11,  1568/ 2400 points] total loss, accuracy per batch: 0.368, 0.893\n","[Epoch: 11,  1792/ 2400 points] total loss, accuracy per batch: 0.370, 0.888\n","[Epoch: 11,  2016/ 2400 points] total loss, accuracy per batch: 0.340, 0.888\n","[Epoch: 11,  2240/ 2400 points] total loss, accuracy per batch: 0.403, 0.857\n","12/22/2021 10:22:35 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:06<00:00,  1.45it/s]\n","12/22/2021 10:22:41 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:22:41 AM [INFO]:   accuracy = 0.8451388888888889\n","12/22/2021 10:22:41 AM [INFO]:   f1 = 0.25000000000000006\n","12/22/2021 10:22:41 AM [INFO]:   precision = 0.4\n","12/22/2021 10:22:41 AM [INFO]:   recall = 0.18181818181818182\n","Epoch finished, took 66.58 seconds.\n","Losses at Epoch 11: 0.3612770\n","Train accuracy at Epoch 11: 0.8839286\n","Test f1 at Epoch 11: 0.2500000\n","[Epoch: 12,   224/ 2400 points] total loss, accuracy per batch: 0.281, 0.915\n","[Epoch: 12,   448/ 2400 points] total loss, accuracy per batch: 0.304, 0.906\n","[Epoch: 12,   672/ 2400 points] total loss, accuracy per batch: 0.323, 0.906\n","[Epoch: 12,   896/ 2400 points] total loss, accuracy per batch: 0.427, 0.871\n","[Epoch: 12,  1120/ 2400 points] total loss, accuracy per batch: 0.374, 0.871\n","[Epoch: 12,  1344/ 2400 points] total loss, accuracy per batch: 0.409, 0.871\n","[Epoch: 12,  1568/ 2400 points] total loss, accuracy per batch: 0.337, 0.893\n","[Epoch: 12,  1792/ 2400 points] total loss, accuracy per batch: 0.325, 0.871\n","[Epoch: 12,  2016/ 2400 points] total loss, accuracy per batch: 0.404, 0.862\n","[Epoch: 12,  2240/ 2400 points] total loss, accuracy per batch: 0.344, 0.875\n","12/22/2021 10:23:41 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:06<00:00,  1.49it/s]\n","12/22/2021 10:23:47 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:23:47 AM [INFO]:   accuracy = 0.8458333333333333\n","12/22/2021 10:23:47 AM [INFO]:   f1 = 0.2777777777777778\n","12/22/2021 10:23:47 AM [INFO]:   precision = 0.4166666666666667\n","12/22/2021 10:23:47 AM [INFO]:   recall = 0.20833333333333334\n","Epoch finished, took 64.50 seconds.\n","Losses at Epoch 12: 0.3527097\n","Train accuracy at Epoch 12: 0.8839286\n","Test f1 at Epoch 12: 0.2777778\n","[Epoch: 13,   224/ 2400 points] total loss, accuracy per batch: 0.265, 0.942\n","[Epoch: 13,   448/ 2400 points] total loss, accuracy per batch: 0.317, 0.906\n","[Epoch: 13,   672/ 2400 points] total loss, accuracy per batch: 0.306, 0.897\n","[Epoch: 13,   896/ 2400 points] total loss, accuracy per batch: 0.288, 0.920\n","[Epoch: 13,  1120/ 2400 points] total loss, accuracy per batch: 0.281, 0.915\n","[Epoch: 13,  1344/ 2400 points] total loss, accuracy per batch: 0.319, 0.902\n","[Epoch: 13,  1568/ 2400 points] total loss, accuracy per batch: 0.327, 0.875\n","[Epoch: 13,  1792/ 2400 points] total loss, accuracy per batch: 0.389, 0.875\n","[Epoch: 13,  2016/ 2400 points] total loss, accuracy per batch: 0.347, 0.915\n","[Epoch: 13,  2240/ 2400 points] total loss, accuracy per batch: 0.313, 0.902\n","12/22/2021 10:24:48 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:06<00:00,  1.44it/s]\n","12/22/2021 10:24:54 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:24:54 AM [INFO]:   accuracy = 0.8527777777777777\n","12/22/2021 10:24:54 AM [INFO]:   f1 = 0.35294117647058826\n","12/22/2021 10:24:54 AM [INFO]:   precision = 0.6\n","12/22/2021 10:24:54 AM [INFO]:   recall = 0.25\n","Epoch finished, took 64.61 seconds.\n","Losses at Epoch 13: 0.3150250\n","Train accuracy at Epoch 13: 0.9049107\n","Test f1 at Epoch 13: 0.3529412\n","[Epoch: 14,   224/ 2400 points] total loss, accuracy per batch: 0.341, 0.902\n","[Epoch: 14,   448/ 2400 points] total loss, accuracy per batch: 0.248, 0.933\n","[Epoch: 14,   672/ 2400 points] total loss, accuracy per batch: 0.310, 0.906\n","[Epoch: 14,   896/ 2400 points] total loss, accuracy per batch: 0.363, 0.888\n","[Epoch: 14,  1120/ 2400 points] total loss, accuracy per batch: 0.361, 0.888\n","[Epoch: 14,  1344/ 2400 points] total loss, accuracy per batch: 0.317, 0.888\n","[Epoch: 14,  1568/ 2400 points] total loss, accuracy per batch: 0.302, 0.911\n","[Epoch: 14,  1792/ 2400 points] total loss, accuracy per batch: 0.297, 0.897\n","[Epoch: 14,  2016/ 2400 points] total loss, accuracy per batch: 0.281, 0.911\n","[Epoch: 14,  2240/ 2400 points] total loss, accuracy per batch: 0.389, 0.866\n","12/22/2021 10:25:58 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:06<00:00,  1.46it/s]\n","12/22/2021 10:26:04 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:26:04 AM [INFO]:   accuracy = 0.875\n","12/22/2021 10:26:04 AM [INFO]:   f1 = 0.14285714285714288\n","12/22/2021 10:26:04 AM [INFO]:   precision = 0.3333333333333333\n","12/22/2021 10:26:04 AM [INFO]:   recall = 0.09090909090909091\n","Epoch finished, took 64.89 seconds.\n","Losses at Epoch 14: 0.3208379\n","Train accuracy at Epoch 14: 0.8991071\n","Test f1 at Epoch 14: 0.1428571\n","[Epoch: 15,   224/ 2400 points] total loss, accuracy per batch: 0.248, 0.924\n","[Epoch: 15,   448/ 2400 points] total loss, accuracy per batch: 0.291, 0.906\n","[Epoch: 15,   672/ 2400 points] total loss, accuracy per batch: 0.284, 0.929\n","[Epoch: 15,   896/ 2400 points] total loss, accuracy per batch: 0.322, 0.906\n","[Epoch: 15,  1120/ 2400 points] total loss, accuracy per batch: 0.329, 0.893\n","[Epoch: 15,  1344/ 2400 points] total loss, accuracy per batch: 0.312, 0.915\n","[Epoch: 15,  1568/ 2400 points] total loss, accuracy per batch: 0.306, 0.897\n","[Epoch: 15,  1792/ 2400 points] total loss, accuracy per batch: 0.254, 0.938\n","[Epoch: 15,  2016/ 2400 points] total loss, accuracy per batch: 0.292, 0.915\n","[Epoch: 15,  2240/ 2400 points] total loss, accuracy per batch: 0.296, 0.915\n","12/22/2021 10:27:04 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.59it/s]\n","12/22/2021 10:27:10 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:27:10 AM [INFO]:   accuracy = 0.8638888888888889\n","12/22/2021 10:27:10 AM [INFO]:   f1 = 0.21428571428571427\n","12/22/2021 10:27:10 AM [INFO]:   precision = 0.42857142857142855\n","12/22/2021 10:27:10 AM [INFO]:   recall = 0.14285714285714285\n","Epoch finished, took 63.15 seconds.\n","Losses at Epoch 15: 0.2933859\n","Train accuracy at Epoch 15: 0.9138393\n","Test f1 at Epoch 15: 0.2142857\n","[Epoch: 16,   224/ 2400 points] total loss, accuracy per batch: 0.359, 0.893\n","[Epoch: 16,   448/ 2400 points] total loss, accuracy per batch: 0.199, 0.929\n","[Epoch: 16,   672/ 2400 points] total loss, accuracy per batch: 0.285, 0.920\n","[Epoch: 16,   896/ 2400 points] total loss, accuracy per batch: 0.292, 0.902\n","[Epoch: 16,  1120/ 2400 points] total loss, accuracy per batch: 0.310, 0.915\n","[Epoch: 16,  1344/ 2400 points] total loss, accuracy per batch: 0.259, 0.924\n","[Epoch: 16,  1568/ 2400 points] total loss, accuracy per batch: 0.276, 0.920\n","[Epoch: 16,  1792/ 2400 points] total loss, accuracy per batch: 0.278, 0.915\n","[Epoch: 16,  2016/ 2400 points] total loss, accuracy per batch: 0.337, 0.897\n","[Epoch: 16,  2240/ 2400 points] total loss, accuracy per batch: 0.335, 0.902\n","12/22/2021 10:28:15 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.55it/s]\n","12/22/2021 10:28:21 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:28:21 AM [INFO]:   accuracy = 0.8527777777777777\n","12/22/2021 10:28:21 AM [INFO]:   f1 = 0.3243243243243243\n","12/22/2021 10:28:21 AM [INFO]:   precision = 0.4\n","12/22/2021 10:28:21 AM [INFO]:   recall = 0.2727272727272727\n","Epoch finished, took 66.07 seconds.\n","Losses at Epoch 16: 0.2928523\n","Train accuracy at Epoch 16: 0.9116071\n","Test f1 at Epoch 16: 0.3243243\n","[Epoch: 17,   224/ 2400 points] total loss, accuracy per batch: 0.234, 0.933\n","[Epoch: 17,   448/ 2400 points] total loss, accuracy per batch: 0.307, 0.888\n","[Epoch: 17,   672/ 2400 points] total loss, accuracy per batch: 0.261, 0.893\n","[Epoch: 17,   896/ 2400 points] total loss, accuracy per batch: 0.342, 0.888\n","[Epoch: 17,  1120/ 2400 points] total loss, accuracy per batch: 0.280, 0.933\n","[Epoch: 17,  1344/ 2400 points] total loss, accuracy per batch: 0.208, 0.929\n","[Epoch: 17,  1568/ 2400 points] total loss, accuracy per batch: 0.302, 0.911\n","[Epoch: 17,  1792/ 2400 points] total loss, accuracy per batch: 0.331, 0.888\n","[Epoch: 17,  2016/ 2400 points] total loss, accuracy per batch: 0.224, 0.929\n","[Epoch: 17,  2240/ 2400 points] total loss, accuracy per batch: 0.330, 0.902\n","12/22/2021 10:29:22 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:06<00:00,  1.47it/s]\n","12/22/2021 10:29:29 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:29:29 AM [INFO]:   accuracy = 0.875\n","12/22/2021 10:29:29 AM [INFO]:   f1 = 0.23529411764705882\n","12/22/2021 10:29:29 AM [INFO]:   precision = 0.4\n","12/22/2021 10:29:29 AM [INFO]:   recall = 0.16666666666666666\n","Epoch finished, took 64.96 seconds.\n","Losses at Epoch 17: 0.2818146\n","Train accuracy at Epoch 17: 0.9093750\n","Test f1 at Epoch 17: 0.2352941\n","[Epoch: 18,   224/ 2400 points] total loss, accuracy per batch: 0.294, 0.902\n","[Epoch: 18,   448/ 2400 points] total loss, accuracy per batch: 0.256, 0.924\n","[Epoch: 18,   672/ 2400 points] total loss, accuracy per batch: 0.267, 0.924\n","[Epoch: 18,   896/ 2400 points] total loss, accuracy per batch: 0.300, 0.911\n","[Epoch: 18,  1120/ 2400 points] total loss, accuracy per batch: 0.373, 0.879\n","[Epoch: 18,  1344/ 2400 points] total loss, accuracy per batch: 0.243, 0.920\n","[Epoch: 18,  1568/ 2400 points] total loss, accuracy per batch: 0.241, 0.933\n","[Epoch: 18,  1792/ 2400 points] total loss, accuracy per batch: 0.304, 0.911\n","[Epoch: 18,  2016/ 2400 points] total loss, accuracy per batch: 0.285, 0.924\n","[Epoch: 18,  2240/ 2400 points] total loss, accuracy per batch: 0.221, 0.938\n","12/22/2021 10:30:30 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:06<00:00,  1.47it/s]\n","12/22/2021 10:30:36 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:30:36 AM [INFO]:   accuracy = 0.8486111111111111\n","12/22/2021 10:30:36 AM [INFO]:   f1 = 0.31249999999999994\n","12/22/2021 10:30:36 AM [INFO]:   precision = 0.45454545454545453\n","12/22/2021 10:30:36 AM [INFO]:   recall = 0.23809523809523808\n","Epoch finished, took 65.29 seconds.\n","Losses at Epoch 18: 0.2783521\n","Train accuracy at Epoch 18: 0.9165179\n","Test f1 at Epoch 18: 0.3125000\n","[Epoch: 19,   224/ 2400 points] total loss, accuracy per batch: 0.251, 0.924\n","[Epoch: 19,   448/ 2400 points] total loss, accuracy per batch: 0.306, 0.893\n","[Epoch: 19,   672/ 2400 points] total loss, accuracy per batch: 0.240, 0.924\n","[Epoch: 19,   896/ 2400 points] total loss, accuracy per batch: 0.253, 0.906\n","[Epoch: 19,  1120/ 2400 points] total loss, accuracy per batch: 0.314, 0.902\n","[Epoch: 19,  1344/ 2400 points] total loss, accuracy per batch: 0.267, 0.924\n","[Epoch: 19,  1568/ 2400 points] total loss, accuracy per batch: 0.263, 0.902\n","[Epoch: 19,  1792/ 2400 points] total loss, accuracy per batch: 0.313, 0.888\n","[Epoch: 19,  2016/ 2400 points] total loss, accuracy per batch: 0.308, 0.920\n","[Epoch: 19,  2240/ 2400 points] total loss, accuracy per batch: 0.228, 0.933\n","12/22/2021 10:31:39 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.51it/s]\n","12/22/2021 10:31:45 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:31:45 AM [INFO]:   accuracy = 0.8597222222222222\n","12/22/2021 10:31:45 AM [INFO]:   f1 = 0.35294117647058826\n","12/22/2021 10:31:45 AM [INFO]:   precision = 0.6\n","12/22/2021 10:31:45 AM [INFO]:   recall = 0.25\n","Epoch finished, took 64.45 seconds.\n","Losses at Epoch 19: 0.2742667\n","Train accuracy at Epoch 19: 0.9116071\n","Test f1 at Epoch 19: 0.3529412\n","[Epoch: 20,   224/ 2400 points] total loss, accuracy per batch: 0.251, 0.924\n","[Epoch: 20,   448/ 2400 points] total loss, accuracy per batch: 0.228, 0.924\n","[Epoch: 20,   672/ 2400 points] total loss, accuracy per batch: 0.297, 0.915\n","[Epoch: 20,   896/ 2400 points] total loss, accuracy per batch: 0.226, 0.933\n","[Epoch: 20,  1120/ 2400 points] total loss, accuracy per batch: 0.229, 0.933\n","[Epoch: 20,  1344/ 2400 points] total loss, accuracy per batch: 0.338, 0.884\n","[Epoch: 20,  1568/ 2400 points] total loss, accuracy per batch: 0.275, 0.920\n","[Epoch: 20,  1792/ 2400 points] total loss, accuracy per batch: 0.302, 0.911\n","[Epoch: 20,  2016/ 2400 points] total loss, accuracy per batch: 0.298, 0.893\n","[Epoch: 20,  2240/ 2400 points] total loss, accuracy per batch: 0.306, 0.911\n","12/22/2021 10:32:47 AM [INFO]: Evaluating test samples...\n","100% 9/9 [00:05<00:00,  1.61it/s]\n","12/22/2021 10:32:52 AM [INFO]: ***** Eval results *****\n","12/22/2021 10:32:52 AM [INFO]:   accuracy = 0.8673611111111111\n","12/22/2021 10:32:52 AM [INFO]:   f1 = 0.30303030303030304\n","12/22/2021 10:32:52 AM [INFO]:   precision = 0.4166666666666667\n","12/22/2021 10:32:52 AM [INFO]:   recall = 0.23809523809523808\n","Epoch finished, took 65.04 seconds.\n","Losses at Epoch 20: 0.2749758\n","Train accuracy at Epoch 20: 0.9147321\n","Test f1 at Epoch 20: 0.3030303\n","12/22/2021 10:32:55 AM [INFO]: Finished Training!\n","12/22/2021 10:33:12 AM [INFO]: Loading tokenizer and model...\n","12/22/2021 10:33:13 AM [INFO]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","12/22/2021 10:33:13 AM [INFO]: Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","12/22/2021 10:33:13 AM [INFO]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","Model config:  {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","12/22/2021 10:33:17 AM [INFO]: Weights of BertModel not initialized from pretrained model: ['bert.classification_layer.weight', 'bert.classification_layer.bias']\n","12/22/2021 10:33:19 AM [INFO]: Loaded checkpoint model.\n","12/22/2021 10:33:19 AM [INFO]: Loaded model and optimizer.\n","12/22/2021 10:33:19 AM [INFO]: Done!\n"]}],"source":["# Train the model\n","# --train_data: Full path of the trainning dataset\n","# --test_data: Full path of the testing dataset\n","\n","%cd '/content/drive/MyDrive/GitHub/BERT-Relation-Extraction'\n","!CUDA_LAUNCH_BLOCKING=1 python main_task.py --train_data=\"/content/drive/MyDrive/GitHub/train_data_dm1.txt\" --test_data=\"/content/drive/MyDrive/GitHub/test_data_dm1.txt\" --num_epochs=20"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83077,"status":"ok","timestamp":1640169461501,"user":{"displayName":"Tsz Chung Yeung","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17478010975126361284"},"user_tz":-480},"id":"mhK3SG57ZCiP","outputId":"71eeefe2-8e9b-4eb6-bbeb-79868d632599"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1jqlh-dTe4b6_AZhAYDBHkKjWweeOh9HE/COMP4805 Project/Go to Street/__Evergrande/BERT-Relation-Extraction\n","12/22/2021 10:36:38 AM [INFO]: Loading tokenizer and model...\n","12/22/2021 10:36:46 AM [INFO]: TensorFlow version 2.7.0 available.\n","12/22/2021 10:36:46 AM [INFO]: PyTorch version 1.10.0+cu111 available.\n","12/22/2021 10:36:47 AM [INFO]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","12/22/2021 10:36:47 AM [INFO]: Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","12/22/2021 10:36:47 AM [INFO]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","Model config:  {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","12/22/2021 10:36:50 AM [INFO]: Weights of BertModel not initialized from pretrained model: ['bert.classification_layer.weight', 'bert.classification_layer.bias']\n","12/22/2021 10:37:09 AM [INFO]: Loaded checkpoint model.\n","12/22/2021 10:37:09 AM [INFO]: Loaded model and optimizer.\n","12/22/2021 10:37:09 AM [INFO]: Done!\n","Input a xlsx file, the model will generate a new excel folder with relations.\n","Please input the path of data: /content/drive/MyDrive/COMP4805 Project/Go to Street/__Tencent/TestingData_Tencent.xlsx\n","Sentence:  SHANGHAI, Dec 17 ([E1]Reuters[/E1]) - Chinese regulators have given [E2]Tencent Holdings[/E2] (0700.HK) approval to publish updates to nine of its mobile apps including QQ Music and WeCom, media reported on Friday.\n","Predicted:  Other \n","\n","Other\n","Sentence:  SHANGHAI, Dec 17 ([E1]Reuters[/E1]) - Chinese regulators have given Tencent Holdings (0700.HK) approval to publish updates to nine of its mobile apps including [E2]QQ Music and WeCom[/E2], media reported on Friday.\n","Predicted:  Other \n","\n","Other\n","Sentence:  SHANGHAI, Dec 17 (Reuters) - Chinese regulators have given [E1]Tencent Holdings[/E1] (0700.HK) approval to publish updates to nine of its mobile apps including [E2]QQ Music and WeCom[/E2], media reported on Friday.\n","Predicted:  Other \n","\n","█ Part of the output data are hidden for copyright protection █\n","\n","Subsidary-ParentCompany(e1,e2)\n","Sentence:  As [E1]Advance Data Services Limited[/E1] is wholly-owned by [E2]Ma[/E2] Huateng, Mr Ma has an interest in these shares as disclosed under the section of “Directors’ Interests in Securities”.\n","Predicted:  Subsidary-ParentCompany(e1,e2) \n","\n","Subsidary-ParentCompany(e1,e2)\n","Sentence:  As Advance Data Services Limited is wholly-owned by [E2]Ma[/E2][E1]Ma Huateng[/E1], Mr Ma has an interest in these shares as disclosed under the section of “Directors’ Interests in Securities”.\n","Predicted:  Other \n","\n","Other\n","Sentence:  [E1]The Audit Committee[/E1], together with the Auditor, has reviewed the [E2]Group[/E2]’s audited consolidated financial statements for the year ended 31 December 2020. The Audit Committee has also reviewed the accounting principles and practices adopted by the Group and discussed auditing, risk management, internal control and financial reporting matters.\n","Predicted:  Other \n","\n","Other\n","Sentence:  As far as the [E2]Board[/E2] is aware, the [E1]Group[/E1] has complied with the relevant laws and regulations that have a significant impact on the Group in all material respects.\n","Predicted:  Other \n","\n","Other\n","Generating output to result.xlsx...\n","Done!\n"]}],"source":["# Run the pretrained model\n","# Return the predicted result with mapping\n","# Input: Path of the dataset, in .xlsx format\n","\n","%cd \"/content/drive/MyDrive/GitHub/BERT-Relation-Extraction\""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Tencent_RE BERT model.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
